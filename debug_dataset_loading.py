#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë¡œë”© ë””ë²„ê¹… ë° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python debug_dataset_loading.py --config-file configs/speaq_multi_dataset.yaml
"""

import argparse
import sys
import os
import json
import h5py
import pickle
import numpy as np
from pathlib import Path

# Detectron2 ë° í”„ë¡œì íŠ¸ ì„í¬íŠ¸
from detectron2.config import get_cfg
from detectron2.data import DatasetCatalog, MetadataCatalog

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from data.tools import register_datasets
from configs.defaults import add_dataset_config, add_scenegraph_config


def analyze_h5_structure(h5_path):
    """H5 íŒŒì¼ êµ¬ì¡° ë¶„ì„"""
    print("\n" + "="*80)
    print(f"H5 íŒŒì¼ êµ¬ì¡° ë¶„ì„: {h5_path}")
    print("="*80)
    
    if not os.path.exists(h5_path):
        print(f"âš ï¸  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {h5_path}")
        return
    
    with h5py.File(h5_path, 'r') as f:
        print("\nğŸ“ í‚¤ ëª©ë¡:")
        for key in f.keys():
            data = f[key]
            print(f"  - {key}: shape={data.shape}, dtype={data.dtype}")
        
        # Split ë¶„ì„
        if 'split' in f:
            split = f['split'][:]
            print(f"\nğŸ“Š Split ë¶„í¬:")
            print(f"  - Train (0): {np.sum(split == 0)}")
            print(f"  - Val (1): {np.sum(split == 1)}")
            print(f"  - Test (2): {np.sum(split == 2)}")
        
        # ë°•ìŠ¤ ë¶„ì„
        if 'img_to_first_box' in f:
            first_box = f['img_to_first_box'][:]
            valid_images = np.sum(first_box >= 0)
            print(f"\nğŸ“¦ ë°•ìŠ¤ ì •ë³´:")
            print(f"  - ë°•ìŠ¤ê°€ ìˆëŠ” ì´ë¯¸ì§€: {valid_images}")
            print(f"  - ë°•ìŠ¤ê°€ ì—†ëŠ” ì´ë¯¸ì§€: {len(first_box) - valid_images}")
        
        # ê´€ê³„ ë¶„ì„
        if 'img_to_first_rel' in f:
            first_rel = f['img_to_first_rel'][:]
            valid_relations = np.sum(first_rel >= 0)
            print(f"\nğŸ”— ê´€ê³„ ì •ë³´:")
            print(f"  - ê´€ê³„ê°€ ìˆëŠ” ì´ë¯¸ì§€: {valid_relations}")
            print(f"  - ê´€ê³„ê°€ ì—†ëŠ” ì´ë¯¸ì§€: {len(first_rel) - valid_relations}")


def analyze_mapping_dictionary(mapping_path):
    """ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¶„ì„"""
    print("\n" + "="*80)
    print(f"ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¶„ì„: {mapping_path}")
    print("="*80)
    
    if not os.path.exists(mapping_path):
        print(f"âš ï¸  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {mapping_path}")
        return
    
    with open(mapping_path, 'r') as f:
        mapping = json.load(f)
    
    print("\nğŸ“š í´ë˜ìŠ¤ ì •ë³´:")
    if 'label_to_idx' in mapping:
        print(f"  - ê°ì²´ í´ë˜ìŠ¤ ìˆ˜: {len(mapping['label_to_idx'])}")
        print(f"  - ì²« 10ê°œ í´ë˜ìŠ¤:")
        for i, (label, idx) in enumerate(sorted(mapping['label_to_idx'].items(), key=lambda x: x[1])[:10]):
            print(f"    {idx}: {label}")
    
    if 'predicate_to_idx' in mapping:
        print(f"\nğŸ”— ê´€ê³„ í´ë˜ìŠ¤ ìˆ˜: {len(mapping['predicate_to_idx'])}")
        print(f"  - ì²« 10ê°œ ê´€ê³„:")
        for i, (pred, idx) in enumerate(sorted(mapping['predicate_to_idx'].items(), key=lambda x: x[1])[:10]):
            print(f"    {idx}: {pred}")
    
    if 'attribute_to_idx' in mapping:
        print(f"\nğŸ·ï¸  ì†ì„± í´ë˜ìŠ¤ ìˆ˜: {len(mapping['attribute_to_idx'])}")


def analyze_dataset_dicts(dataset_name, sample_count=5):
    """ë°ì´í„°ì…‹ dict ë¶„ì„"""
    print("\n" + "="*80)
    print(f"ë°ì´í„°ì…‹ ë¶„ì„: {dataset_name}")
    print("="*80)
    
    if dataset_name not in DatasetCatalog.list():
        print(f"âš ï¸  ë°ì´í„°ì…‹ì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {dataset_name}")
        return
    
    dataset_dicts = DatasetCatalog.get(dataset_name)
    print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´:")
    print(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset_dicts)}")
    
    if len(dataset_dicts) == 0:
        print("  âš ï¸  ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        return
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ ë¶„ì„
    sample = dataset_dicts[0]
    print(f"\nğŸ“ ìƒ˜í”Œ êµ¬ì¡°:")
    print(f"  - í‚¤: {list(sample.keys())}")
    print(f"  - ì´ë¯¸ì§€ ID: {sample.get('image_id', 'N/A')}")
    print(f"  - íŒŒì¼ëª…: {sample.get('file_name', 'N/A')}")
    print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {sample.get('width', 'N/A')} x {sample.get('height', 'N/A')}")
    print(f"  - ê°ì²´ ìˆ˜: {len(sample.get('annotations', []))}")
    print(f"  - ê´€ê³„ ìˆ˜: {len(sample.get('relations', []))}")
    
    # ê°ì²´ í†µê³„
    if 'annotations' in sample and len(sample['annotations']) > 0:
        obj = sample['annotations'][0]
        print(f"\nğŸ“¦ ê°ì²´ êµ¬ì¡°:")
        print(f"  - í‚¤: {list(obj.keys())}")
        print(f"  - bbox: {obj.get('bbox', 'N/A')}")
        print(f"  - category_id: {obj.get('category_id', 'N/A')}")
        print(f"  - attribute shape: {np.array(obj.get('attribute', [])).shape}")
    
    # ê´€ê³„ í†µê³„
    if 'relations' in sample and len(sample['relations']) > 0:
        relations = np.array(sample['relations'])
        print(f"\nğŸ”— ê´€ê³„ êµ¬ì¡°:")
        print(f"  - shape: {relations.shape}")
        print(f"  - ì²« 5ê°œ ê´€ê³„:")
        for i, rel in enumerate(relations[:5]):
            print(f"    {i}: subject={rel[0]}, object={rel[1]}, predicate={rel[2]}")
    
    # ì „ì²´ í†µê³„
    num_objs = [len(d['annotations']) for d in dataset_dicts[:1000]]  # ì²˜ìŒ 1000ê°œë§Œ
    num_rels = [len(d['relations']) for d in dataset_dicts[:1000]]
    
    print(f"\nğŸ“ˆ í†µê³„ (ì²˜ìŒ 1000ê°œ ìƒ˜í”Œ):")
    print(f"  - í‰ê·  ê°ì²´ ìˆ˜: {np.mean(num_objs):.2f}")
    print(f"  - ìµœëŒ€ ê°ì²´ ìˆ˜: {np.max(num_objs)}")
    print(f"  - í‰ê·  ê´€ê³„ ìˆ˜: {np.mean(num_rels):.2f}")
    print(f"  - ìµœëŒ€ ê´€ê³„ ìˆ˜: {np.max(num_rels)}")


def analyze_cache_files():
    """ìºì‹œ íŒŒì¼ ë¶„ì„"""
    print("\n" + "="*80)
    print("ìºì‹œ íŒŒì¼ ë¶„ì„")
    print("="*80)
    
    cache_dir = Path("tmp")
    if not cache_dir.exists():
        print("  âš ï¸  ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    cache_files = list(cache_dir.glob("*.pkl"))
    print(f"\nğŸ“¦ ìºì‹œ íŒŒì¼ ìˆ˜: {len(cache_files)}")
    
    for cache_file in cache_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ
        file_size = cache_file.stat().st_size / (1024 * 1024)  # MB
        print(f"  - {cache_file.name}: {file_size:.2f} MB")
        
        # ìºì‹œ íŒŒì¼ì—ì„œ ìƒ˜í”Œ ì •ë³´ í™•ì¸
        try:
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
                if isinstance(data, list) and len(data) > 0:
                    print(f"    â†’ ìƒ˜í”Œ ìˆ˜: {len(data)}")
                    if isinstance(data[0], dict):
                        print(f"    â†’ í‚¤: {list(data[0].keys())}")
        except Exception as e:
            print(f"    âš ï¸  ë¡œë“œ ì‹¤íŒ¨: {e}")


def analyze_metadata(dataset_name):
    """ë©”íƒ€ë°ì´í„° ë¶„ì„"""
    print("\n" + "="*80)
    print(f"ë©”íƒ€ë°ì´í„° ë¶„ì„: {dataset_name}")
    print("="*80)
    
    if dataset_name not in MetadataCatalog.list():
        print(f"âš ï¸  ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
        return
    
    metadata = MetadataCatalog.get(dataset_name)
    print(f"\nğŸ“š í´ë˜ìŠ¤ ì •ë³´:")
    
    if hasattr(metadata, 'thing_classes'):
        print(f"  - ê°ì²´ í´ë˜ìŠ¤ ìˆ˜: {len(metadata.thing_classes)}")
        print(f"  - ì²« 10ê°œ: {metadata.thing_classes[:10]}")
    
    if hasattr(metadata, 'predicate_classes'):
        print(f"  - ê´€ê³„ í´ë˜ìŠ¤ ìˆ˜: {len(metadata.predicate_classes)}")
        print(f"  - ì²« 10ê°œ: {metadata.predicate_classes[:10]}")
    
    if hasattr(metadata, 'attribute_classes'):
        print(f"  - ì†ì„± í´ë˜ìŠ¤ ìˆ˜: {len(metadata.attribute_classes)}")
    
    if hasattr(metadata, 'statistics'):
        stats = metadata.statistics
        print(f"\nğŸ“Š í†µê³„ ì •ë³´:")
        print(f"  - fg_rel_count shape: {stats['fg_rel_count'].shape}")
        print(f"  - fg_rel_count sum: {stats['fg_rel_count'].sum():.0f}")
        print(f"  - fg_matrix shape: {stats['fg_matrix'].shape}")
        print(f"  - ê°€ì¥ ë¹ˆë²ˆí•œ ê´€ê³„ (ìƒìœ„ 10ê°œ):")
        top_rels = torch.topk(stats['fg_rel_count'], 10)
        for i, (value, idx) in enumerate(zip(top_rels.values, top_rels.indices)):
            if idx < len(metadata.predicate_classes):
                print(f"    {i+1}. {metadata.predicate_classes[idx]}: {value:.0f}")


def analyze_multi_dataset(cfg):
    """ë©€í‹° ë°ì´í„°ì…‹ ë¶„ì„"""
    print("\n" + "="*80)
    print("ë©€í‹° ë°ì´í„°ì…‹ ì„¤ì • ë¶„ì„")
    print("="*80)
    
    if cfg.DATASETS.TYPE != "MULTI_DATASET":
        print("  âš ï¸  ë©€í‹° ë°ì´í„°ì…‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return
    
    multi_cfg = cfg.DATASETS.MULTI_DATASET
    print(f"\nâš™ï¸  ì„¤ì •:")
    print(f"  - Enabled: {multi_cfg.ENABLED}")
    print(f"  - Real sampling ratio: {multi_cfg.REAL_SAMPLING_RATIO}")
    print(f"  - Synthetic sampling ratio: {multi_cfg.SYNTHETIC_SAMPLING_RATIO}")
    print(f"  - Real loss weight: {multi_cfg.REAL_LOSS_WEIGHT}")
    print(f"  - Synthetic loss weight: {multi_cfg.SYNTHETIC_LOSS_WEIGHT}")
    
    # Real ë°ì´í„°ì…‹ ë¶„ì„
    if 'VG_train' in DatasetCatalog.list():
        real_dicts = DatasetCatalog.get('VG_train')
        print(f"\nğŸ“Š Real ë°ì´í„°ì…‹:")
        print(f"  - í¬ê¸°: {len(real_dicts)}")
    
    # Synthetic ë°ì´í„°ì…‹ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
    # Multi-datasetì—ì„œëŠ” ë³„ë„ë¡œ ë“±ë¡ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
    if 'MULTI_train' in DatasetCatalog.list():
        multi_dicts = DatasetCatalog.get('MULTI_train')
        print(f"\nğŸ“Š Multi ë°ì´í„°ì…‹:")
        print(f"  - í¬ê¸°: {len(multi_dicts)}")
        if hasattr(multi_dicts, 'real_size'):
            print(f"  - Real size: {multi_dicts.real_size}")
            print(f"  - Synthetic size: {multi_dicts.synthetic_size}")


def main():
    parser = argparse.ArgumentParser(description='ë°ì´í„°ì…‹ ë¡œë”© ë””ë²„ê¹… ë° ë¶„ì„')
    parser.add_argument('--config-file', type=str, required=True,
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--analyze-h5', action='store_true',
                       help='H5 íŒŒì¼ êµ¬ì¡° ë¶„ì„')
    parser.add_argument('--analyze-mapping', action='store_true',
                       help='ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¶„ì„')
    parser.add_argument('--analyze-dataset', type=str, default=None,
                       help='ë¶„ì„í•  ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: VG_train, MULTI_train)')
    parser.add_argument('--analyze-cache', action='store_true',
                       help='ìºì‹œ íŒŒì¼ ë¶„ì„')
    parser.add_argument('--analyze-all', action='store_true',
                       help='ëª¨ë“  ë¶„ì„ ìˆ˜í–‰')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    print("="*80)
    print("ë°ì´í„°ì…‹ ë¡œë”© ë””ë²„ê¹… ë° ë¶„ì„")
    print("="*80)
    print(f"\nì„¤ì • íŒŒì¼: {args.config_file}")
    
    cfg = get_cfg()
    add_dataset_config(cfg)
    add_scenegraph_config(cfg)
    cfg.merge_from_file(args.config_file)
    cfg.freeze()
    
    # ë°ì´í„°ì…‹ ë“±ë¡
    print("\në°ì´í„°ì…‹ ë“±ë¡ ì¤‘...")
    register_datasets(cfg)
    print(f"ë“±ë¡ëœ ë°ì´í„°ì…‹: {DatasetCatalog.list()}")
    
    # ë¶„ì„ ìˆ˜í–‰
    if args.analyze_all:
        args.analyze_h5 = True
        args.analyze_mapping = True
        args.analyze_cache = True
        if cfg.DATASETS.TYPE == "MULTI_DATASET":
            args.analyze_dataset = "MULTI_train"
        elif cfg.DATASETS.TYPE == "VISUAL GENOME":
            args.analyze_dataset = "VG_train"
        elif cfg.DATASETS.TYPE == "SYNTHETIC GENOME":
            args.analyze_dataset = "SYNTHETIC_train"
    
    # H5 íŒŒì¼ ë¶„ì„
    if args.analyze_h5:
        if cfg.DATASETS.TYPE == "VISUAL GENOME":
            h5_path = cfg.DATASETS.VISUAL_GENOME.VG_ATTRIBUTE_H5
            analyze_h5_structure(h5_path)
        elif cfg.DATASETS.TYPE == "SYNTHETIC GENOME":
            h5_path = cfg.DATASETS.SYNTHETIC_GENOME.SYNTHETIC_ATTRIBUTE_H5
            analyze_h5_structure(h5_path)
        elif cfg.DATASETS.TYPE == "MULTI_DATASET":
            # Real ë°ì´í„°ì…‹
            h5_path = cfg.DATASETS.VISUAL_GENOME.VG_ATTRIBUTE_H5
            analyze_h5_structure(h5_path)
            # Synthetic ë°ì´í„°ì…‹
            h5_path_syn = cfg.DATASETS.VISUAL_GENOME_SYNTHETIC.VG_ATTRIBUTE_H5
            analyze_h5_structure(h5_path_syn)
    
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë¶„ì„
    if args.analyze_mapping:
        if cfg.DATASETS.TYPE == "VISUAL GENOME":
            mapping_path = cfg.DATASETS.VISUAL_GENOME.MAPPING_DICTIONARY
            analyze_mapping_dictionary(mapping_path)
        elif cfg.DATASETS.TYPE == "SYNTHETIC GENOME":
            mapping_path = cfg.DATASETS.SYNTHETIC_GENOME.MAPPING_DICTIONARY
            analyze_mapping_dictionary(mapping_path)
        elif cfg.DATASETS.TYPE == "MULTI_DATASET":
            mapping_path = cfg.DATASETS.VISUAL_GENOME.MAPPING_DICTIONARY
            analyze_mapping_dictionary(mapping_path)
    
    # ìºì‹œ íŒŒì¼ ë¶„ì„
    if args.analyze_cache:
        analyze_cache_files()
    
    # ë°ì´í„°ì…‹ ë¶„ì„
    if args.analyze_dataset:
        analyze_dataset_dicts(args.analyze_dataset)
        analyze_metadata(args.analyze_dataset)
    
    # ë©€í‹° ë°ì´í„°ì…‹ ë¶„ì„
    if cfg.DATASETS.TYPE == "MULTI_DATASET":
        analyze_multi_dataset(cfg)
    
    print("\n" + "="*80)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    import torch
    main()

